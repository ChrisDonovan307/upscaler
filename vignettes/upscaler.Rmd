---
title: "upscaler"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{upscaler}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(upscaler)
```

# Introduction to `upscaler`
The `upscaler` package grew out of the need to teach Computational Biology students how to move beyond "single script" programming and build larger programs that operate on multiple files and call on separate scripts. No matter how large or small the project, code should be organized to create a reproducible research pipeline, so that all of the numerical input and output for a project can be reproduced, and the work can be reliably repeated if there are changes to the data or extensions of the code. `upscaler` follows the general principles of functional programming, providing a small collection of R functions to reduce overhead and more easily document code and procedures. 

This vignette describes how to use each of the functions available in `upscaler` to improve the organization and reproducability of your project. 

# Acknowledgements
Package development was supported by NSF grant 2019470 RII Track-2 FEC : Harnessing Spatiotemporal Data Science to Predict Responses of Biodiversity and Rural Communities under Climate Change (co-PIs Brian J. McGill and Nicholas J. Gotelli). The functions were first presented at a Data Science Skill-building Bootcamp offered through the grant in June of 2022. I thank Alex Burnham, Leo Edminster-Cyr, Laurent Hebert-Dufrense, Matt Dube, Matt Kling, Bailey McLaughlin, Brian McGill, Tim Waring, and the entire `barracuda` team (**B**iodiversity **a**nd **R**ural **R**esponse to **C**limate **C**hange **U**sing **D**ata **A**nalysis) for advice and improvements to the code.


# Helper & Template Functions

## Organize your project with `addfolder()`

The first step in building an organized project is to structure it with a useful set of subfolders. The default pointer in R is always to the root of a project folder, so we have a tendency to dump everything in there, and it quickly becomes a cluttered storage area. 

First, make sure you have loaded upscaler into memory by issuing the following command from the console:

```{r, eval=FALSE}
library(upscaler)
```

Next, move to the root of the project you are working in and give the following command, again from the console:

```{r, eval=FALSE}
add_folder()
```
This will create a set of useful folders to store the different pieces of your work. If you don't like the default folder titles, just supply your own:

```{r, eval=FALSE}
add_folder(c("myfolderA","myfolderB","myfolderC"))

```

Each "empty" folder created by `add_folder()` actually contains an empty (and hidden) file called `.keep`. The reason for this is to get around the unexpected behavior of github, which will not preserve a folder in your repository unless it contains at least one file or object. This happens because github only recognizes folder names to the extent that they are part of the relative path to an object that is stored in the repository. As a safety precaution, `add_folder()` always checks to make sure the folder it is about to add does not already exist in the repository, in which case it will abort and give a warning message that the folder already exists. So you will never accidentally over-write anything by using `add_folder()`.

Here are some comments on how to use each of the folders created by default with `add_folder`.

### `OriginalData` Folder

This folder should contain the original data files, in whatever native file type you receive them. That is the only thing that should be placed in this folder. If you are dealing with large data sets that cannot be stored locally, then you need to include a plain text file that has the appropriate link and whatever other information is needed to construct a breadcrumb trail that will take you back to the data you are using. An important principle you should adhere to is to never ever edit or modify anything once you put it in this folder. You also will never read or use any of these objects. Instead, you will first copy them to:

### `CleanedData` Folder

Here is where the work begins of converting these files from their original "dirty" state into a "clean" state. Clean files are those that can be opened with a `read.table()` command in R. Getting files into the clean state sometimes takes a lot of work, and it can be hard to document your changes carefully, especially if you are using powerful tools like regular expressions. So you may want to include a plain-text file that explains what you did to modify these files from their source versions that are stored in the `OriginalData` folder.

You should make minimal changes in this part of your work, just enough so that the files can be opened. In your other scripts, you will probably need to wrangle these data to get them into the correct layout for analysis (especially common is the need to convert from the wide to the long format). And even when you have them opened and in the correct format, you need to check them to make sure they do not contain bad data or problems such as errant characters in columns of numeric data, or incorrectly coded or labeled NA values. Again, these kinds of changes can be difficult to properly document, but do your best so that, if someone started over with the files from the `OriginalData` folder, they could get them into the state that you have them in the `CleanData` folder. And remember, don't edit anything in the `OriginalData` folder. Always work with a copy placed in the `CleanData` folder before you start doing anything. 

### `Scripts` Folder

Here is where you will keep the primary R scripts for your project. People's coding styles differ greatly, and you can use as many or as few scripts as you like to get the work done. You can never have too much annotation in those scripts, so CTRL-SHIFT-C should become your new best friend as you write and debug your code. Scripts that are created by your collaborators should also be stored here, possibly with some other folder structure if it starts getting too messy. 

Personally, I like to use a single `MainScript.R` that I store in the root of the project. This script calls all of the libraries I use, sources all of the functions, and executes any individual script files contained in these folders. Remember that since the R is pointing to the root of your project, it is easy to source scripts inside your subfolder with this format:

```{r, eval=FALSE}
source("Scripts/MyScript.R")
```

As you will see, `MainScript.R` is also a good place to set up the log-file system that is part of `upscaler`. If you do it this way, no matter what changes or additions you make to your project, you can recreate everything just by sourcing `MainScript.R` without having to worry about which of the individual scripts need to be run again.

### `Functions` Folder

I like to create a separate script for every function I build, and store them all here. This really promotes functional programming and breaking down a complex task into distinct pieces, coding those elements in isolation, and then linking them all up to build a complex program. The main disadvantage of this approach is the overhead and time associated with setting up and naming individual function files. As you will see, `upscaler` gives you some tools (`build_function()` and `source_batch()`) that will eliminate the repetitive work.

### `Plots` Folder

Use this folder to store all of the graphic output files (.png,.pdf.tiff, etc.) that are created by your scripts. You should always use `ggsave()` and other commands to create graphic output rather than using the GUI interface (which is simpler, but does not make your work fully reproducible). 

### `Outputs` Folder

Any .csv files or other non-graphic objects that are created by your code should be placed here. For manuscript assembly, I also like to put in here a .csv version of any table that is going in to my manuscript, even one that contains only text (see `data_table_template()`). In this way, all the pieces of the publication live and travel together in the R project. As a general rule, you should avoid generating and transmitting .csv data files other than those that you set up in the `CleanedData` folder.

### `DataObjects` Folder

In a large project, you are often sharing data and other objects with other programmers. As long as everyone is working in R, it is better to store objects as an RDS file ("R Serialized Data") than to create a .csv. Use this folder to store RDS objects that you create or that are sent to you by others for your project. RDS objects can be created with `saveRDS(r_object,"object_name.rds")` and opened into an object with a specified name with `restored_object <- readRDS("object_name.rds")`. Remember that the `saveRDS()` function operates on only a single object, so if you have multiple items you want to pass along, bundle them first into a list.


### `Markdown` Folder

Markdown lets you conveniently mix R code with formatted text in a .Rmd file, and then render it to a pdf or html document (or .docx, if you insist). It is convenient for sharing updates with collaborators. However, you may not like to do the actual programming in an .Rmd document because your code is broken up into chunks. Use this folder to keep all of your .Rmd documents together. The rendered file will also be placed in this folder, so it will not add clutter to your script and function folders.

## Creating a set of padded labels with `create_padded_labels()`

If you are creating a large number of objects or file names that will be written to disk, it is convenient to have padded zeroes in the names so that they are correctly sorted by your computer's file explorer. This function lets you specify the number of labels you want to create, a text string for the prefix, and an optional text string for the suffix. The output is a vector of character strings. For example:

```{r}
z <- create_padded_labels(n=10,string="Species",suffix=".txt")
print(z)
```

## Create a csv file for a data frame with `data_table_template()`

When you are working on a project from which you will create a manuscript draft, it is a good idea to keep all of the tables and the graphs together in your project. The `data_table_template()` lets you import any data frame (even one that just contains text) and convert it to a .csv. You specify to the function the name of your data frame and the name of the output file. For example:

```{r, eval=FALSE}
data_table_template(data_frame=my_data,file_name="Table_A.txt")

```

If you call this function with no inputs (`data_table_template()`), it creates a file called `ToyDataTable.csv` which looks like this:

```
# Table X. Table legend
# 
# timestamp: 04 July 2024 15:12:57
# ------------------------
# 
"Species","Control","Low","High"
"Species_A",0.757,0.111,0.577
"Species_B",0.991,0.691,0.192
```

Since R will strip out the lines that start with a hash tag, this .csv file can be opened with `read.table()`. But the more important use is to store in your project of all the tabled data (text or numbers) that will eventually appear as elements in your manuscript.

## Embed your critical meta data within your .csv file with `metadata_template()`

An epic challenge in science is recording sufficient meta-data so that, in the future, you and others can understand and use the data that you have collected in the past. In the past, researchers agonized over how meta-data could be recorded in a universal way that would be accessible to others, but data sets are unique and defy the best efforts to create templates that are useful. AI is hepful here because we can now use free-form text narratives, written in the authors' native language, that can then be captured by data mining. Public data repositories always have a place to store your meta-data, but the fatal flaw is that those meta-data are often not stored in the same object as the data themselves. When that happens, the chain is broken because the data are worthless without the file containing the metadata.

The upscaler function `metadata_template` creates and timestamps a simple .csv file with some suggested elements that you typically need to properly annotate a data frame. This material lives behind hash tags, so both the meta-data and the data themselves will live together in the same .csv file. You should freely modify this template to suit your own needs, but the important thing is to have that meta-data embedded in the .csv file. A bit of editing will probably also be needed to open this file with `read.table()`, but the far more time-consuming task will be writing the meta-data descriptions.

The only input you need to use `metadata_template()` is the file name. Use it like this:

```{r,eval=FALSE}
metadata_template("MyAnnotatedData.csv")
```

If you don't supply the file name, the function will create a file called "MetaDataTemplate.csv". Here is what it contains:

```
#//////////////////////////////
#---START OF METADATA---
#------------------------------
# TITLE: Brief title for entire data set
# CREATION TIMESTAMP: 04 July 2024 16:05:46
# AUTHOR: author/owner of data
#------------------------------
# AUTHOR EMAIL:
# AUTHOR ADDRESS:
# AUTHOR WEBSITE:
#------------------------------
# OWNERSHIP: Information on who owns/controls the data

# COLLABORATORS: One or more lines identifying others who collected/own data
# FUNDING SOURCES: Grant numbers or funding sources for acknowlegment
# REPOSITORY: One or more lines for GitHub, Dryad, or permanent web repositories where data set can be accessed
# CITATIONS: One or more lines for publications that cite or use these data

#------------------------------
# SAMPLING LOCATIONS: One or more lines about where data were collected; GPS for individual sites should be data columns
# SAMPLING TIMES: One more lines on when data were collected
# VARIABLE DESCRIPTION: One line for each column in the data set stating what it is and what the units of measurement are
# MISSING DATA: One or more lines for each variable describing the source of NA values throughout
#------------------------------
# DATA TRACK CHANGES LOG (use this section to record any changes to the data set after it is created)
# DATE:,CHANGES:	
# DATE:,CHANGES:	
# DATE:,CHANGES:	
# DATE:,CHANGES:	
#------------------------------
#---END OF METADATA---
#//////////////////////////////
#
#
#---START OF DATA---
#
ID,VarOne,VarTwo,VarThree,TextNotes



# Can insert comments throughout text to add notes for individual data rows


#---END OF DATA---
```

This file can be successfully opened with `read.table()`, but only the single line beginning with ID,VarOne... is actually read in. That row would function as the list of comma-separated variable names, and each subsequent row would contain the comma-separated data entries.

